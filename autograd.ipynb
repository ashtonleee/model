{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56a0e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: SGD / Adam optimizer\n",
    "# optimizer class?\n",
    "# inference mode?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "bca7dd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "from collections.abc import Callable\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "e99d6a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Function:\n",
    "    def __init__(self, func: Callable, deriv: Callable):\n",
    "        self.func = func\n",
    "        self.deriv = deriv\n",
    "\n",
    "# sigmoid\n",
    "def sigmoid(x: np.array):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def dsigmoid(x: np.array):\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x))\n",
    "\n",
    "# softmax\n",
    "def softmax(x: np.array):\n",
    "    x = x - np.max(x)\n",
    "    exp = np.exp(x)\n",
    "    return exp / np.sum(exp)\n",
    "\n",
    "def SMCE(y: np.array, y_pred: np.array):\n",
    "    '''softmax cross entropy loss'''\n",
    "    sm = softmax(y_pred)\n",
    "    return -np.sum(y * np.log(sm))\n",
    "\n",
    "def dSMCE(y: np.array, y_pred: np.array):\n",
    "    '''derivative of softmax cross entropy loss'''\n",
    "    return softmax(y_pred) - y\n",
    "\n",
    "# mse\n",
    "def MSE(y: np.array, y_pred: np.array):\n",
    "    return math.sqrt(np.sum((y_pred - y) ** 2))\n",
    "\n",
    "def dMSE(y: np.array, y_pred: np.array):\n",
    "    return 2 * (y_pred - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "b1195716",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, n: int, m: int, activation: Function):\n",
    "        self.W = np.random.rand(n, m)\n",
    "        self.b = np.zeros((m))\n",
    "        self.activation = activation\n",
    "\n",
    "        self.x = None\n",
    "        self.z = None\n",
    "        self.s = None\n",
    "\n",
    "    def _forward(self, x: np.array) -> np.array:\n",
    "        # f(xW + b)\n",
    "        self.x = x\n",
    "        self.z = x @ self.W + self.b\n",
    "        self.s = self.activation.deriv(self.z)\n",
    "        return self.activation.func(self.z)\n",
    "    \n",
    "    def _backward(self, upstream: np.array) -> np.array:\n",
    "        # dL/dz = dL/df hadamard df/dz = upstream H df/dz; (1xm) H (1xm)\n",
    "        delta = upstream * self.s\n",
    "        # dL/dx = dL/dz @ dz/dx\n",
    "        dx = delta @ self.W.T\n",
    "        # dL/db = dL/dz @ dz/db = dL/dz @ I_m\n",
    "        db = np.mean(delta, axis=0)\n",
    "        # dL/dW = dL/dz outer prod dz/dW (do row by row if batch)\n",
    "        #dW = np.einsum('ij,ik->jk', self.x, delta) / np.shape(self.x)\n",
    "        #dW = np.outer(self.x.T, delta)\n",
    "        dW = self.x.T @ delta\n",
    "        if self.x.ndim == 2:\n",
    "            dW /= self.x.shape[0]\n",
    "        return dx, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771ef003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, layers: List[Layer] = None):\n",
    "        self.layers = [] if layers is None else layers\n",
    "        self.gradient = [None] * len(self.layers)\n",
    "        self.logits = None\n",
    "        self.dL = 1.0\n",
    "\n",
    "    def forward(self, x: np.array) -> np.array:\n",
    "        '''forward pass'''\n",
    "        h = x\n",
    "        for layer in self.layers:\n",
    "            h = layer._forward(h)\n",
    "        self.logits = h\n",
    "        return self.logits\n",
    "\n",
    "    def backward(self):\n",
    "        '''calc gradient via backpropagation'''\n",
    "        # dL/df (1xn gradient)\n",
    "        upstream = self.dL\n",
    "        for i, layer in enumerate(reversed(self.layers)):\n",
    "            dx, dW, db = layer._backward(upstream)\n",
    "            self.gradient[-i-1] = [dW, db]\n",
    "            upstream = dx\n",
    "\n",
    "    def step(self, lr: float):\n",
    "        '''lr = learning rate'''\n",
    "        for layer, grad in zip(self.layers, self.gradient):\n",
    "            layer.W -= grad[0] * lr\n",
    "            layer.b -= grad[1] * lr\n",
    "\n",
    "    def train_step(self, x: np.array, y: np.array, loss: Function, lr: float = 0.001, verbose: bool = False):\n",
    "        '''X is just a single sample'''\n",
    "        y_pred = self.forward(x)\n",
    "        train_loss = loss.func(y, y_pred)\n",
    "        self.dL = loss.deriv(y, y_pred)\n",
    "        self.backward()\n",
    "        self.step(lr)\n",
    "\n",
    "        if verbose:\n",
    "            print(f'x: {x}, y: {y}')\n",
    "            print(f'Predicted y: {y_pred}')\n",
    "            # print(f'Gradient: {self.gradient}')\n",
    "        return train_loss\n",
    "    \n",
    "    def train_batch(self, X: np.array, Y: np.array, loss: Function, lr: float = 0.001, verbose: bool = False):\n",
    "        '''X: pxn, Y: pxm'''\n",
    "    \n",
    "        Y_pred = self.forward(X)\n",
    "        # mean loss across batch\n",
    "        train_loss = np.mean([loss.func(y, y_pred) for y, y_pred in zip(Y, Y_pred)])\n",
    "        self.dL = np.array([loss.deriv(y, y_pred) for y, y_pred in zip(Y, Y_pred)])\n",
    "\n",
    "        self.backward()\n",
    "        self.step(lr)\n",
    "\n",
    "        if verbose:\n",
    "            print(f'x: {X}, y: {Y}')\n",
    "            print(f'Predicted y: {Y_pred}')\n",
    "        return train_loss\n",
    "    \n",
    "    def train(self, X: np.array, Y: np.array, loss: Function, epochs: int, lr: float = 0.001, verbose: bool = False):\n",
    "        n = 5\n",
    "        for i in range(epochs):\n",
    "            train_loss = self.train_batch(X, Y, loss, lr=lr, verbose=verbose)\n",
    "            if i % (epochs // n) == 0:\n",
    "                print(f'Epoch {i}:')\n",
    "                print(f'Loss: {train_loss}')\n",
    "                print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "c1b11c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sa leads to nonsense\n",
    "# Activation Functions:\n",
    "identity = Function(lambda x: x, lambda x: np.ones_like(x))\n",
    "relu = Function(lambda x: np.maximum(x, 0), lambda x: np.where(x > 0, 1, 0))\n",
    "sigmoidf = Function(sigmoid, dsigmoid)\n",
    "\n",
    "# Loss Functions:\n",
    "cross_entropy = Function(SMCE, dSMCE)\n",
    "MSE_loss = Function(MSE, dMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "de0f9627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "Loss: 0.4976219642263848\n",
      "Epoch 20000:\n",
      "Loss: 0.49586164059081506\n",
      "Epoch 40000:\n",
      "Loss: 0.24805294256299415\n",
      "Epoch 60000:\n",
      "Loss: 2.1407220027025442e-05\n",
      "Epoch 80000:\n",
      "Loss: 6.668153251787601e-10\n"
     ]
    }
   ],
   "source": [
    "# XOR Model:\n",
    "layers = [Layer(2, 2, sigmoidf), Layer(2, 1, identity)]\n",
    "XOR_model = Model(layers)\n",
    "\n",
    "# data\n",
    "X = np.array([[1, 0], [0, 1], [1, 1], [0, 0]])\n",
    "Y = np.array([[1], [1], [0], [0]])\n",
    "\n",
    "XOR_model.train(X, Y, MSE_loss, 100000, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "3281c8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "Loss: 0.515232797735823\n",
      "Epoch 200000:\n",
      "Loss: 0.28892203735680694\n",
      "Epoch 400000:\n",
      "Loss: 2.780747213371182e-05\n",
      "Epoch 600000:\n",
      "Loss: 7.904616960985322e-10\n",
      "Epoch 800000:\n",
      "Loss: 1.1994016890781722e-12\n"
     ]
    }
   ],
   "source": [
    "# XOR Model:\n",
    "layers = [Layer(2, 2, sigmoidf), Layer(2, 1, relu)]\n",
    "XOR_model2 = Model(layers)\n",
    "\n",
    "# data\n",
    "X = np.array([[1, 0], [0, 1], [1, 1], [0, 0]])\n",
    "Y = np.array([[1], [1], [0], [0]])\n",
    "\n",
    "XOR_model2.train(X, Y, MSE_loss, 1000000, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "70cd0807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "Loss: 0.672345147442822\n",
      "Epoch 1000:\n",
      "Loss: 0.047115901025098925\n",
      "Epoch 2000:\n",
      "Loss: 0.047437749842675146\n",
      "Epoch 3000:\n",
      "Loss: 0.0472471467132408\n",
      "Epoch 4000:\n",
      "Loss: 0.047088673370622426\n"
     ]
    }
   ],
   "source": [
    "# Multiplication Model\n",
    "layers = [Layer(2, 2, relu), Layer(2, 1, relu)]\n",
    "mult_model = Model(layers)\n",
    "\n",
    "# data\n",
    "X = np.random.rand(3000,2)\n",
    "Y = np.array([x[0] * x[1] for x in X])\n",
    "\n",
    "mult_model.train(X, Y, MSE_loss, 5000, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "ee7bb71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _multiply(x, y):\n",
    "    return mult_model.forward(np.array([x, y]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "d0aaf5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1]\n",
      "[1 1 1]\n",
      "[[12. 12. 12.]\n",
      " [15. 15. 15.]\n",
      " [18. 18. 18.]]\n",
      "[[12. 12. 12.]\n",
      " [15. 15. 15.]\n",
      " [18. 18. 18.]]\n",
      "[[ 6.  6.  6.]\n",
      " [15. 15. 15.]\n",
      " [24. 24. 24.]]\n",
      "[[0 0 0]\n",
      " [0 1 1]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "b = np.ones((3, 3))\n",
    "\n",
    "x = [1, 1, 1]\n",
    "print(x)\n",
    "print(np.stack(x))\n",
    "print(np.einsum('ij,ik->jk', a, b))\n",
    "print(sum(np.outer(A, B) for A, B in zip(a, b)))\n",
    "print(a @ b)\n",
    "print(np.where(a > 4, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "3af89e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.3966454920348912)"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_multiply(0.5, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2868577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
