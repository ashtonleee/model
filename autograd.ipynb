{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca7dd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "from collections.abc import Callable\n",
    "import operator as op\n",
    "\n",
    "import math\n",
    "from collections import deque, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1195716",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Function:\n",
    "    def __init__(self, func: Callable, deriv: Callable):\n",
    "        self.func = func\n",
    "        self.deriv = deriv\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, n: int, m: int, activation: Function):\n",
    "        self.W = np.random.rand(n, m)\n",
    "        self.b = np.zeros((m))\n",
    "        self.activation = activation\n",
    "        self.adjoint = self.W.T\n",
    "        self.cache = np.random.rand(n)\n",
    "\n",
    "    def _forward(self, x: np.array) -> np.array:\n",
    "        # f(xW + b)\n",
    "        self.cache = x\n",
    "        a = x @ self.W + self.b\n",
    "        return self.activation.func(a)\n",
    "    \n",
    "    def _backward(self, upstream: np.array) -> np.array:\n",
    "        # dL/dz = dL/df @ df/dz = upstream @ df/dz; (1xm) @ (mxm)\n",
    "        delta = upstream @ self.activation.deriv(self.cache)\n",
    "        # dL/dx = dL/dz @ dz/dx\n",
    "        dx = delta @ self.adjoint\n",
    "        # dL/db = dL/dz @ dz/db = dL/dz @ I_m\n",
    "        db = delta\n",
    "        # dL/dW = dL/dz @ dz/dW\n",
    "        dW = self.cache.T @ delta\n",
    "        return dx\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, N: int, M: int, layers: List[Layer] = None):\n",
    "        self.layers = [] if layers is None else layers\n",
    "        self.N = N\n",
    "        self.M = M\n",
    "\n",
    "    def forward(self, input: np.array) -> np.array:\n",
    "        x = input\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, loss: Function):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
