{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bca7dd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "from collections.abc import Callable\n",
    "import operator as op\n",
    "\n",
    "import math\n",
    "from collections import deque, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e99d6a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Function:\n",
    "    def __init__(self, func: Callable, deriv: Callable):\n",
    "        self.func = func\n",
    "        self.deriv = deriv\n",
    "\n",
    "def softmax(x: np.array):\n",
    "    x = x - np.max(x)\n",
    "    exp = np.exp(x)\n",
    "    return exp / np.sum(exp)\n",
    "\n",
    "def sigmoid(x: np.array):\n",
    "    return np.vectorize(lambda n: 1 / (1 + math.exp(-n)))(x)\n",
    "\n",
    "def dsigmoid(x: np.array):\n",
    "    return np.multiply(sigmoid(x), 1 - sigmoid(x))\n",
    "\n",
    "def SMCE_loss(y: np.array, y_pred: np.array):\n",
    "    '''softmax cross entropy loss'''\n",
    "    sm = softmax(y_pred)\n",
    "    return -np.sum(y * np.log(sm))\n",
    "\n",
    "def dSMCE_loss(y: np.array, y_pred: np.array):\n",
    "    '''derivative of softmax cross entropy loss'''\n",
    "    return softmax(y_pred) - y\n",
    "\n",
    "def softmax_cross_entropy(y: np.array):\n",
    "    SMCE_y = lambda y_pred: SMCE_loss(y, y_pred)\n",
    "    dSMCE_y = lambda y_pred: dSMCE_loss(y, y_pred)\n",
    "    return Function(SMCE_y, dSMCE_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b1195716",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, n: int, m: int, activation: Function):\n",
    "        self.W = np.random.rand(n, m)\n",
    "        self.b = np.zeros((m))\n",
    "        self.activation = activation\n",
    "        self.adjoint = self.W.T\n",
    "\n",
    "        self.x = None\n",
    "        self.z = None\n",
    "        self.s = None\n",
    "\n",
    "    def _forward(self, x: np.array) -> np.array:\n",
    "        # f(xW + b)\n",
    "        self.x = x\n",
    "        self.z = x @ self.W + self.b\n",
    "        # cache df/dz\n",
    "        self.s = self.activation.deriv(self.z)\n",
    "        return self.activation.func(self.z)\n",
    "    \n",
    "    def _backward(self, upstream: np.array) -> np.array:\n",
    "        # dL/dz = dL/df hadamard df/dz = upstream H df/dz; (1xm) H (1xm)\n",
    "        delta = upstream * self.s\n",
    "        # dL/dx = dL/dz @ dz/dx\n",
    "        dx = delta @ self.adjoint\n",
    "        # dL/db = dL/dz @ dz/db = dL/dz @ I_m\n",
    "        db = delta\n",
    "        # dL/dW = dL/dz outer prod dz/dW (kronecker?)\n",
    "        dW = np.outer(self.x.T, delta)\n",
    "        return dx, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "771ef003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, layers: List[Layer] = None):\n",
    "        self.layers = [] if layers is None else layers\n",
    "        self.gradient = []\n",
    "        self.logits = None\n",
    "        self.loss = None\n",
    "\n",
    "    def forward(self, x: np.array) -> np.array:\n",
    "        '''forward pass'''\n",
    "        h = x\n",
    "        for layer in self.layers:\n",
    "            h = layer._forward(h)\n",
    "        self.logits = h\n",
    "        return self.logits\n",
    "\n",
    "    def backward(self):\n",
    "        '''calc gradient via backpropagation'''\n",
    "        self.gradient = []\n",
    "        # dL/df (1xn gradient)\n",
    "        upstream = self.loss.deriv(self.logits)\n",
    "        for layer in reversed(self.layers):\n",
    "            dx, dW, db = layer._backward(upstream)\n",
    "            self.gradient.append([dW, db])\n",
    "            upstream = dx\n",
    "\n",
    "    def step(self, lr: float):\n",
    "        '''lr = learning rate'''\n",
    "        for layer, grad in zip(reversed(self.layers), self.gradient):\n",
    "            layer.W -= grad[0] * lr\n",
    "            layer.b -= grad[1] * lr\n",
    "\n",
    "    def train(self, X: np.array, Y: np.array, loss_func: Callable, lr: float = 0.01, verbose: bool = False):\n",
    "        '''X is just a single sample'''\n",
    "        Y_pred = self.forward(X)\n",
    "        self.loss = loss_func(Y)\n",
    "        loss = self.loss.func(Y_pred)\n",
    "        self.backward()\n",
    "        self.step(lr)\n",
    "\n",
    "        if verbose:\n",
    "            print(f'X: {X}, Y: {Y}')\n",
    "            print(f'Predicted Y: {Y_pred}')\n",
    "            print(f'Softmax: {softmax(Y_pred)}')\n",
    "            print(f'Gradient: {self.gradient}')\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c1b11c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0:\n",
      "Loss: 0.6935463524195231\n",
      "[[0.6598274  0.89516147]\n",
      " [0.30639388 0.19657315]]\n",
      "[[0.56537038 0.57399178]\n",
      " [0.94923346 0.72692567]]\n",
      "iter 10000:\n",
      "Loss: 0.6928576635889179\n",
      "[[0.75162921 0.89251676]\n",
      " [0.49005319 0.14736567]]\n",
      "[[0.36830138 0.81732841]\n",
      " [0.86498721 0.85585391]]\n",
      "iter 20000:\n",
      "Loss: 0.6835823595581908\n",
      "[[1.50283865 0.98883192]\n",
      " [1.4822224  0.24803416]]\n",
      "[[-0.25186493  1.55870496]\n",
      " [ 0.94503983  0.8575983 ]]\n",
      "iter 30000:\n",
      "Loss: 0.5864175345268844\n",
      "[[4.33393123 1.42715722]\n",
      " [4.31085657 0.48266008]]\n",
      "[[-2.8344788   4.15742743]\n",
      " [ 2.0718235   0.16556694]]\n",
      "iter 40000:\n",
      "Loss: 0.44646543245386616\n",
      "[[5.99237493 2.70297   ]\n",
      " [5.91934214 2.68447719]]\n",
      "[[-5.17279716  5.3646836 ]\n",
      " [ 5.50640819 -2.28932059]]\n",
      "iter 50000:\n",
      "Loss: 0.35131080971853357\n",
      "[[6.66581385 4.20535851]\n",
      " [6.62887016 4.202488  ]]\n",
      "[[-6.79978046  6.64814086]\n",
      " [ 7.40003969 -6.10723835]]\n",
      "iter 60000:\n",
      "Loss: 0.3343122137889377\n",
      "[[6.99516204 4.78452821]\n",
      " [6.96899139 4.7820549 ]]\n",
      "[[-7.67995715  7.50831778]\n",
      " [ 8.30960128 -7.57709693]]\n",
      "iter 70000:\n",
      "Loss: 0.3278076684242409\n",
      "[[7.21327025 5.11475338]\n",
      " [7.19246669 5.11256319]]\n",
      "[[-8.27913838  8.11667324]\n",
      " [ 8.92067355 -8.4172533 ]]\n",
      "iter 80000:\n",
      "Loss: 0.3243660121027031\n",
      "[[7.37500383 5.34109171]\n",
      " [7.35746317 5.33912405]]\n",
      "[[-8.73437477  8.58561356]\n",
      " [ 9.38222759 -9.00017603]]\n",
      "iter 90000:\n",
      "Loss: 0.3222352638890716\n",
      "[[7.50268458 5.51142428]\n",
      " [7.48735691 5.50963284]]\n",
      "[[-9.10189516  8.96625477]\n",
      " [ 9.75325516 -9.44570388]]\n"
     ]
    }
   ],
   "source": [
    "sa = Function(lambda x: x/2, lambda x: 1/2)\n",
    "sigmoidf = Function(sigmoid, dsigmoid)\n",
    "layers = [Layer(2, 2, sigmoidf), Layer(2, 2, sigmoidf)]\n",
    "model = Model(layers)\n",
    "\n",
    "\n",
    "training = [[1,0], [0,1], [1,1], [0,0]]\n",
    "# train\n",
    "steps = 100000\n",
    "lr = 0.01\n",
    "for i in range(steps):\n",
    "    avg_loss = 0\n",
    "    for x in training:\n",
    "        ans = np.array([0,0])\n",
    "        ans[x[0] ^ x[1]] = 1\n",
    "        avg_loss += model.train(np.array(x), ans, loss_func = softmax_cross_entropy, lr = lr) / 4\n",
    "    if i % (steps // 10) == 0:\n",
    "        print(f'iter {i}:')\n",
    "        print(f'Loss: {avg_loss}')\n",
    "        for layer in model.layers:\n",
    "            print(layer.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0f9627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aaf5ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
