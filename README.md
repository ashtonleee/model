autodiff = basic autodifferentiation

autograd = basic PyTorch autograd engine: linear, sequential layers

transformer = LLaMA-2 inspired modern transformer with RoPE, MHA, SwiGLU activations
