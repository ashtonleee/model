{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46dff0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0be503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Map:\n",
    "\n",
    "# Transformer = Tokenizer -> Embedder -> Attention Block (Nx) -> Norm -> Linear -> Softmax\n",
    "# Tokenizer = BPE / import\n",
    "# Embedder = random weights\n",
    "# Attn Block = RMSNorm + MHA + (add residual x) + RMSNorm + FFN (MLP) + (add residual x)\n",
    "# RMSNorm = x / (RMS(x) + eps)\n",
    "# MHA = project x to W_i, Q_i, K_i and ship to Attn Head i -> concat results *W_o -> out\n",
    "# Attn Head i = [Q, K = RoPE(Q, K)] -> softmax((QK^T) / sqrt(d/H) + M) V -> out\n",
    "# MLP = (linear -> activation) (Nx) -> out\n",
    "\n",
    "# TODO:\n",
    "#   DTYPE configs?\n",
    "#   general dtype enforcement\n",
    "#   remove global params\n",
    "\n",
    "\n",
    "# CONFIGS:\n",
    "# d = dim(embedding), T = #toks, H = #heads, B = batch size\n",
    "# d = 1024\n",
    "# T = 200\n",
    "# H = 8\n",
    "# B = 1\n",
    "\n",
    "# d_ff = 2048 # d_ff = dim(FFN hidden layers)\n",
    "# MAXT = 4098 # max tokens\n",
    "# V = 4098 # vocab size\n",
    "# RoPE_base = 10000 # RoPE base (just need sufficiently large?)\n",
    "\n",
    "# f32 = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "02318b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    # TODO:\n",
    "    #   SwiGLU X\n",
    "    #   support n layers? X (no need)\n",
    "\n",
    "    def __init__(self, d: int, d_ff: int) -> None:\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(d, d_ff)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.up = nn.Linear(d, d_ff)\n",
    "        self.down = nn.Linear(d_ff, d)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.silu(self.gate(x)) * self.up(x)     # SwiGLU : d_ff ~ 2.7d\n",
    "        out = self.down(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0772e7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    # TODO:\n",
    "    #   RoPE X\n",
    "    #   KV cache\n",
    "    #   update mask? X\n",
    "    #   matmuls? X\n",
    "    #   require_grad?\n",
    "    \n",
    "    def __init__(self, d: int, H: int, maxT: int) -> None:\n",
    "        super().__init__()\n",
    "        # assuming q = v = d/H\n",
    "        self.W_q = nn.Linear(d, d, bias=False)\n",
    "        self.W_k = nn.Linear(d, d, bias=False)\n",
    "        self.W_v = nn.Linear(d, d, bias=False)\n",
    "        self.W_o = nn.Linear(d, d, bias=False)\n",
    "\n",
    "        self.d = d\n",
    "        self.H = H\n",
    "        if d % H:\n",
    "            raise Exception('#heads must divide dim(embedding)')\n",
    "        self.d_h = d // H\n",
    "\n",
    "        # causal mask\n",
    "        self.register_buffer('mask', torch.triu(torch.ones((1, 1, maxT, maxT), dtype=bool), 1))\n",
    "\n",
    "        # angles : (1 x 1 x MAXT x d_h/2)\n",
    "        # thetas : (d_h/2)\n",
    "        if self.d_h & 1:\n",
    "            raise Exception('dim(head) must be even')\n",
    "        RoPE_base = 10000   # just need sufficiently large?\n",
    "        self.register_buffer('theta', RoPE_base ** (-2 * torch.arange(self.d_h // 2, dtype=torch.float32) / self.d_h))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X : (B x T x d)\n",
    "        B, T, d = X.shape\n",
    "        if d != self.d:\n",
    "            raise Exception('dim(input) does not match dim(model)')\n",
    "\n",
    "        # Q, K, V : (B x T x d)\n",
    "        Q, K, V = self.W_q(X), self.W_k(X), self.W_v(X)\n",
    "\n",
    "        # reshape into heads [row: [tok: [dh: [], dh: [], ... ], tok: []] row: ]\n",
    "        # [row: [dh: [tok: [], tok: []], dh: [tok: [], tok: []] row: ]\n",
    "        # Q_h, K_h, V_h : (B x H x T x d_h)\n",
    "        Q_h = torch.reshape(Q, (B, T, self.H, self.d_h)).transpose(1, 2)\n",
    "        K_h = torch.reshape(K, (B, T, self.H, self.d_h)).transpose(1, 2)\n",
    "        V_h = torch.reshape(V, (B, T, self.H, self.d_h)).transpose(1, 2)\n",
    "\n",
    "\n",
    "        # RoPE:\n",
    "        #   x_(i,2j) = cos(angles[i]) * x_(i,2j) - sin(angles[i]) * x_(i,2j+1)\n",
    "        #   x_(i,2j+1) = sin(angles[i]) * x_(i,2j) + cos(angles[i]) * x_(i,2j+1)\n",
    "        angles = torch.outer(torch.arange(T, dtype=torch.float32), self.theta)[None,None,:,:]\n",
    "        cos = torch.cos(angles).to(Q_h.dtype)   # Q_h/K_h.dtype\n",
    "        sin = torch.sin(angles).to(K_h.dtype)\n",
    "\n",
    "        Q_00, Q_10 = Q_h[:,:,:,0::2], Q_h[:,:,:,1::2]\n",
    "        K_00, K_10 = K_h[:,:,:,0::2], K_h[:,:,:,1::2]\n",
    "        Q_01 = cos * Q_00 - sin * Q_10\n",
    "        Q_11 = sin * Q_00 + cos * Q_10\n",
    "        K_01 = cos * K_00 - sin * K_10\n",
    "        K_11 = sin * K_00 + cos * K_10\n",
    "\n",
    "        Q_r = torch.reshape(torch.stack((Q_01, Q_11), dim=-1), (B, self.H, T, self.d_h))\n",
    "        K_r = torch.reshape(torch.stack((K_01, K_11), dim=-1), (B, self.H, T, self.d_h))\n",
    "\n",
    "\n",
    "        # pattern : (B x H x T x T)\n",
    "        # heads : (B x T x H x d_h)\n",
    "        # out : (B x T x d)\n",
    "        scores = Q_r @ K_r.transpose(-2, -1) * self.d_h ** -0.5     # ** -0.5 vs sqrt\n",
    "        scores = scores.masked_fill(self.mask[:,:,:T,:T], torch.finfo(scores.dtype).min)\n",
    "        pattern = torch.softmax(scores, -1)\n",
    "\n",
    "        heads = (pattern @ V_h).transpose(1, 2).contiguous()    # contiguous mem\n",
    "        out = torch.reshape(heads, (B, T, d))\n",
    "        return self.W_o(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "32252200",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, d: int, H: int, MAXT: int, d_ff: int) -> None:\n",
    "        super().__init__()\n",
    "        self.MHA = MultiHeadedAttention(d, H, MAXT)\n",
    "        self.FFN = FeedForwardNetwork(d, d_ff)\n",
    "        self.RMS1 = nn.RMSNorm(d)\n",
    "        self.RMS2 = nn.RMSNorm(d)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # ... : (B x T x d)\n",
    "        out = X + self.MHA(self.RMS1(X))\n",
    "        out = out + self.FFN(self.RMS2(X))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "542e2635",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    # TODO:\n",
    "    #   dropout\n",
    "    #   input params\n",
    "    #   LM head? (bias optional) X\n",
    "    \n",
    "    def __init__(self, d: int, L: int, H: int, MAXT: int, V: int, d_ff: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.L = L\n",
    "        self.H = H\n",
    "        self.MAXT = MAXT\n",
    "        self.V = V\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.embedding = nn.Embedding(V, d)\n",
    "        self.block = AttentionBlock(d, H, MAXT, d_ff)\n",
    "        self.RMS = nn.RMSNorm(d)\n",
    "\n",
    "    def forward(self, X):\n",
    "        B, T = X.shape\n",
    "        if T > self.MAXT:\n",
    "            raise Exception(f'too many tokens; max {self.MAXT} tokens')\n",
    "        \n",
    "        out = self.embedding(X)\n",
    "        out = self.block(out)\n",
    "        out = self.RMS(out)\n",
    "        out = out @ self.embedding.weight.T\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fdf2efdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 4098])\n",
      "tensor([[[ 29.8524,  52.8531,   0.3207,  ...,  15.3518,  -1.9777,   7.7884],\n",
      "         [-20.4772,  56.7334, -27.2075,  ...,  31.8962, -32.9181,  60.6108],\n",
      "         [ -6.1158,   3.7692,  44.1970,  ...,   1.7084,  -9.4435, -38.8689],\n",
      "         ...,\n",
      "         [-44.7907, -53.3638, -30.2178,  ...,   7.3414, -38.8279, -32.0097],\n",
      "         [ -1.1095,   3.2973, -13.3664,  ...,  26.7746,  21.8681,  -8.7413],\n",
      "         [-13.6677,  26.5632,  40.2090,  ..., -19.8639,  49.8688,  13.2879]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "d = 1024        # num dimensions = dim(model)\n",
    "L = 1           # num layers\n",
    "H = 8           # num heads\n",
    "MAXT = 4098     # max tokens\n",
    "V = 4098        # vocab size\n",
    "d_ff = 2048     # dim(FFN hidden layers)\n",
    "\n",
    "transformer = Transformer(d, L, H, MAXT, V, d_ff)\n",
    "X = torch.randint(V, (1, 256))\n",
    "out = transformer(X)\n",
    "print(out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e67ae32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 8.4147e-01,  9.9998e-03,  1.0000e-04,  1.0000e-06],\n",
      "        [ 9.0930e-01,  1.9999e-02,  2.0000e-04,  2.0000e-06],\n",
      "        [ 1.4112e-01,  2.9995e-02,  3.0000e-04,  3.0000e-06],\n",
      "        [-7.5680e-01,  3.9989e-02,  4.0000e-04,  4.0000e-06],\n",
      "        [-9.5892e-01,  4.9979e-02,  5.0000e-04,  5.0000e-06]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.sin(torch.outer(torch.arange(6), 10000 ** (-2 * torch.arange(4) / 4))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e83996e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 5.],\n",
      "         [2., 6.],\n",
      "         [3., 7.],\n",
      "         [4., 8.]],\n",
      "\n",
      "        [[5., 1.],\n",
      "         [6., 2.],\n",
      "         [7., 3.],\n",
      "         [8., 4.]]])\n",
      "tensor([[1., 5., 2., 6., 3., 7., 4., 8.],\n",
      "        [5., 1., 6., 2., 7., 3., 8., 4.]])\n",
      "torch.Size([2, 4])\n",
      "2 4\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "y = torch.Tensor([[5, 6, 7, 8], [1, 2, 3, 4]])\n",
    "print(torch.stack((x,y), dim=-1))\n",
    "print(torch.flatten(torch.stack((x,y), dim=-1), start_dim=-2, end_dim=-1))\n",
    "print(x.shape)\n",
    "a, b = x.shape\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fcbc94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ashton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
