{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46dff0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0be503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Map:\n",
    "\n",
    "# Transformer = Tokenizer -> Embedder -> Attention Block (Nx) -> Norm -> Linear -> Softmax\n",
    "# Tokenizer = BPE / import\n",
    "# Embedder = random weights\n",
    "# Attn Block = RMSNorm + MHA + (add residual x) + RMSNorm + FFN (MLP) + (add residual x)\n",
    "# RMSNorm = x / RMS(x) + eps\n",
    "# MHA = project x to W_i, Q_i, K_i and ship to Attn Head i -> concat results *W_o -> out\n",
    "# Attn Head i = [Q, K = RoPE(Q, K)] -> softmax((QK^T) / sqrt(q) + M) V -> out\n",
    "# MLP = (linear -> activation) (Nx) -> out\n",
    "\n",
    "# i guess we're doing batched\n",
    "\n",
    "\n",
    "# k = dim(embedding), T = #toks, H = #heads, B = batch size, eps for numerical stability\n",
    "k = 1024\n",
    "T = 200\n",
    "H = 8\n",
    "eps = 10 ** -4\n",
    "B = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542e2635",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35498e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    # TODO:\n",
    "    # RoPE\n",
    "    # KV cache\n",
    "    # update mask?\n",
    "    # q, v here or elsewhere; enforce?\n",
    "    # matmuls\n",
    "    # require_grad?\n",
    "\n",
    "    def __init__(self, q, v) -> None:\n",
    "        # q = v = k / H (?)\n",
    "        super().__init__()\n",
    "        self.q = q\n",
    "        self.v = v\n",
    "        self.W_q = nn.Parameter(torch.rand((k, q)))\n",
    "        self.W_k = nn.Parameter(torch.rand((k, q)))\n",
    "        self.W_v = nn.Parameter(torch.rand((k, v)))\n",
    "        self.register_buffer('mask', torch.Tensor([[-math.inf if j > i else 0 for j in range(n)] for i in range(n)]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        Q, K, V = x @ self.W_q, x @ self.W_k, x @ self.W_v\n",
    "        pattern = (Q @ K.T) / math.sqrt(self.q) + self.mask # (nxn)\n",
    "        torch.nn.Softmax(pattern)\n",
    "        return pattern @ V # (nxv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02318b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    # TODO:\n",
    "    # SwiGLU\n",
    "    # support n layers?\n",
    "\n",
    "    def __init__(self, d_ff) -> None:\n",
    "        # project onto d_ff-space\n",
    "        super().__init__()\n",
    "        self.stack = nn.Sequential([nn.Linear(k, d_ff),\n",
    "                                    nn.GELU(),\n",
    "                                    nn.Linear(d_ff, k),\n",
    "                                    nn.GELU()])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0772e7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead(k // H, k // H) for _ in range(H)])\n",
    "        self.W_o = nn.Parameter(torch.rand(k, k))\n",
    "        self.MLP = MLP()\n",
    "        self.RMS_1 = nn.RMSNorm()\n",
    "        self.RMS_2 = nn.RMSNorm()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_norm = self.RMS_1(x)\n",
    "        x_out = torch.cat([head(x_norm) for head in self.heads], dim=1)\n",
    "        y = x + (x_out @ self.W_o)\n",
    "        y_norm = self.RMS_2(y)\n",
    "        y_out = self.MLP(y_norm)\n",
    "        out = x + y_out\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf2efdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [      -inf,       -inf],\n",
       "        [3.7219e+10, 3.6302e+10]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = AttentionHead(2, 2)\n",
    "x = torch.ones((n, k))\n",
    "attn.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e67ae32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ashton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
