{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46dff0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0be503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Map:\n",
    "\n",
    "# Transformer = Tokenizer -> Embedder -> Attention Block (Nx) -> Norm -> Linear -> Softmax\n",
    "# Tokenizer = BPE / import\n",
    "# Embedder = random weights\n",
    "# Attn Block = RMSNorm + MHA + (add residual x) + RMSNorm + FFN (MLP) + (add residual x)\n",
    "# RMSNorm = x / (RMS(x) + eps)\n",
    "# MHA = project x to W_i, Q_i, K_i and ship to Attn Head i -> concat results *W_o -> out\n",
    "# Attn Head i = [Q, K = RoPE(Q, K)] -> softmax((QK^T) / sqrt(d/H) + M) V -> out\n",
    "# MLP = (linear -> activation) (Nx) -> out\n",
    "\n",
    "# i guess we're doing batched\n",
    "\n",
    "\n",
    "# d = dim(embedding), T = #toks, H = #heads, B = batch size, eps for numerical stability\n",
    "d = 1024\n",
    "T = 200\n",
    "H = 8\n",
    "B = 1\n",
    "eps = 10 ** -4\n",
    "\n",
    "d_ff = 2048 # d_ff = dim(FFN hidden layers)\n",
    "MAXT = 4098 # max tokens\n",
    "N = 4098 # vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02318b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    # TODO:\n",
    "    #   SwiGLU\n",
    "    #   support n layers?\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        # project onto d_ff-space\n",
    "        super().__init__()\n",
    "        self.stack = nn.Sequential(nn.Linear(d, d_ff),\n",
    "                                    nn.GELU(),\n",
    "                                    nn.Linear(d_ff, d))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0772e7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    # TODO:\n",
    "    #   RoPE\n",
    "    #   KV cache\n",
    "    #   update mask?\n",
    "    #   matmuls?\n",
    "    #   require_grad?\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # assuming q = v = d/H\n",
    "        self.W_q = nn.Linear(d, d, bias=False)\n",
    "        self.W_k = nn.Linear(d, d, bias=False)\n",
    "        self.W_v = nn.Linear(d, d, bias=False)\n",
    "        self.W_o = nn.Linear(d, d, bias=False)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones((1, 1, MAXT, MAXT)) * -float('inf'), 1)) # torch.finfo(dtype).min\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X, Q, K, V : (B x T x d)\n",
    "        Q, K, V = self.W_q(X), self.W_k(X), self.W_v(X)\n",
    "\n",
    "        # reshape into heads [row: [tok: [dh: [], dh: [], ... ], tok: []] row: ]\n",
    "        # [row: [dh: [tok: [], tok: []], dh: [tok: [], tok: []] row: ]\n",
    "        # Q_h, K_h, V_h : (B x H x T x d_h)\n",
    "        if d % H:\n",
    "            raise Exception('dimension must divide number of heads')\n",
    "        d_h = d // H\n",
    "        Q_h = torch.reshape(Q, (B, T, H, d_h)).transpose(1, 2)\n",
    "        K_h = torch.reshape(K, (B, T, H, d_h)).transpose(1, 2)\n",
    "        V_h = torch.reshape(V, (B, T, H, d_h)).transpose(1, 2)\n",
    "\n",
    "        # pattern : (B x H x T x T)\n",
    "        # heads : (B x T x H x d_h)\n",
    "        # out : (B x T x d)\n",
    "        pattern = torch.softmax(Q_h @ K_h.transpose(2, 3) / math.sqrt(d_h) + self.mask[:,:,:T,:T], -1)\n",
    "        heads = (pattern @ V_h).transpose(1, 2)\n",
    "        out = torch.reshape(heads, (B, T, d))\n",
    "        return self.W_o(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32252200",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.MHA = MultiHeadedAttention()\n",
    "        self.FFN = FeedForwardNetwork()\n",
    "        self.RMS1 = nn.RMSNorm(d)\n",
    "        self.RMS2 = nn.RMSNorm(d)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # X, X_n, X_out, Y, Y_n, Y_out, out : (B x T x d)\n",
    "        X_n = self.RMS1(X)\n",
    "        X_out = self.MHA(X_n)\n",
    "        Y = X + X_out\n",
    "\n",
    "        Y_n = self.RMS2(Y)\n",
    "        Y_out = self.FFN(Y_n)\n",
    "        out = Y + Y_out\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "542e2635",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    # TODO:\n",
    "    #   dropout\n",
    "    #   input params\n",
    "    #   LM head? (bias optional)\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(N, d)\n",
    "        self.block = AttentionBlock()\n",
    "        self.RMS = nn.RMSNorm(d)\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.embedding(X)\n",
    "        out = self.block(out)\n",
    "        out = self.RMS(out)\n",
    "        out = out @ self.embedding.weight.T\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdf2efdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200, 4098])\n",
      "tensor([[[-34.7669,  36.9258,  17.3228,  ..., -12.8140,  69.5626,  18.0535],\n",
      "         [  1.7060,  21.3565,  -6.8058,  ...,   7.1570,  17.1251,  -6.3025],\n",
      "         [  2.7964,  54.0525,  41.5881,  ...,  18.3460,  26.6338,  24.1822],\n",
      "         ...,\n",
      "         [-10.4525, -28.0108, -12.2978,  ...,  30.8323,  45.6175,  13.5453],\n",
      "         [ -3.5123, -62.7142,  -2.9039,  ...,  12.9198,  12.1696, -41.4250],\n",
      "         [ 31.0708,  52.5245,  34.8461,  ..., -22.3652,   1.8458, -29.1236]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer()\n",
    "X = torch.randint(N, (B, T))\n",
    "out = transformer(X)\n",
    "print(out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e67ae32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 3., 5.],\n",
      "         [2., 4., 6.]],\n",
      "\n",
      "        [[1., 3., 5.],\n",
      "         [2., 4., 6.]]])\n"
     ]
    }
   ],
   "source": [
    "test = torch.Tensor([[[1,2],[3,4],[5,6]],[[1,2],[3,4],[5,6]]])\n",
    "print(test.transpose(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83996e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ashton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
