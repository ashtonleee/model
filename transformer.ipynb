{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46dff0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0be503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Map:\n",
    "\n",
    "# Transformer = Tokenizer -> Embedder -> Attention Block (Nx) -> Norm -> Linear -> Softmax\n",
    "# Tokenizer = BPE / import\n",
    "# Embedder = random weights\n",
    "# Attn Block = RMSNorm + MHA + (add residual x) + RMSNorm + FFN (MLP) + (add residual x)\n",
    "# RMSNorm = x / (RMS(x) + eps)\n",
    "# MHA = project x to W_i, Q_i, K_i and ship to Attn Head i -> concat results *W_o -> out\n",
    "# Attn Head i = [Q, K = RoPE(Q, K)] -> softmax((QK^T) / sqrt(d/H) + M) V -> out\n",
    "# MLP = (linear -> activation) (Nx) -> out\n",
    "\n",
    "# TODO:\n",
    "#   DTYPE configs?\n",
    "#   general dtype enforcement\n",
    "#   remove global params\n",
    "\n",
    "\n",
    "\n",
    "# d = dim(embedding), T = #toks, H = #heads, B = batch size, eps for numerical stability\n",
    "d = 1024\n",
    "T = 200\n",
    "H = 8\n",
    "B = 1\n",
    "eps = 10 ** -4\n",
    "\n",
    "d_ff = 2048 # d_ff = dim(FFN hidden layers)\n",
    "MAXT = 4098 # max tokens\n",
    "N = 4098 # vocab size\n",
    "RoPE_base = 10000 # RoPE base (just need sufficiently large?)\n",
    "\n",
    "f32 = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02318b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    # TODO:\n",
    "    #   SwiGLU\n",
    "    #   support n layers?\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        # project onto d_ff-space\n",
    "        super().__init__()\n",
    "        self.stack = nn.Sequential(nn.Linear(d, d_ff),\n",
    "                                    nn.GELU(),\n",
    "                                    nn.Linear(d_ff, d))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0772e7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    # TODO:\n",
    "    #   RoPE X\n",
    "    #   KV cache\n",
    "    #   update mask? X\n",
    "    #   matmuls? X\n",
    "    #   require_grad?\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # assuming q = v = d/H\n",
    "        self.W_q = nn.Linear(d, d, bias=False)\n",
    "        self.W_k = nn.Linear(d, d, bias=False)\n",
    "        self.W_v = nn.Linear(d, d, bias=False)\n",
    "        self.W_o = nn.Linear(d, d, bias=False)\n",
    "\n",
    "        if d % H:\n",
    "            raise Exception('#heads must divide dim(embedding)')\n",
    "        self.d_h = d // H\n",
    "\n",
    "        # causal mask\n",
    "        self.register_buffer('mask', torch.triu(torch.ones((1, 1, MAXT, MAXT), dtype=bool), 1)) # torch.finfo(dtype).min\n",
    "\n",
    "        # angles : (1 x 1 x MAXT x d_h/2)\n",
    "        # thetas : (d_h/2)\n",
    "        if self.d_h & 1:\n",
    "            raise Exception('dim(head) must be even')\n",
    "        \n",
    "        self.register_buffer('theta', RoPE_base ** (-2 * torch.arange(self.d_h // 2, dtype=torch.float32) / self.d_h))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X, Q, K, V : (B x T x d)\n",
    "        Q, K, V = self.W_q(X), self.W_k(X), self.W_v(X)\n",
    "\n",
    "        # reshape into heads [row: [tok: [dh: [], dh: [], ... ], tok: []] row: ]\n",
    "        # [row: [dh: [tok: [], tok: []], dh: [tok: [], tok: []] row: ]\n",
    "        # Q_h, K_h, V_h : (B x H x T x d_h)\n",
    "        Q_h = torch.reshape(Q, (B, T, H, self.d_h)).transpose(1, 2)\n",
    "        K_h = torch.reshape(K, (B, T, H, self.d_h)).transpose(1, 2)\n",
    "        V_h = torch.reshape(V, (B, T, H, self.d_h)).transpose(1, 2)\n",
    "\n",
    "\n",
    "        # RoPE:\n",
    "        #   x_(i,2j) = cos(angles[i]) * x_(i,2j) - sin(angles[i]) * x_(i,2j+1)\n",
    "        #   x_(i,2j+1) = sin(angles[i]) * x_(i,2j) + cos(angles[i]) * x_(i,2j+1)\n",
    "        angles = torch.outer(torch.arange(T, dtype=torch.float32), self.theta)[None,None,:,:]\n",
    "        cos = torch.cos(angles).to(Q_h.dtype)   # Q_h/K_h.dtype\n",
    "        sin = torch.sin(angles).to(K_h.dtype)\n",
    "\n",
    "        Q_00, Q_10 = Q_h[:,:,:,0::2], Q_h[:,:,:,1::2]\n",
    "        K_00, K_10 = K_h[:,:,:,0::2], K_h[:,:,:,1::2]\n",
    "        Q_01 = cos * Q_00 - sin * Q_10\n",
    "        Q_11 = sin * Q_00 + cos * Q_10\n",
    "        K_01 = cos * K_00 - sin * K_10\n",
    "        K_11 = sin * K_00 + cos * K_10\n",
    "\n",
    "        Q_r = torch.flatten(torch.stack((Q_01, Q_11), dim=-1), start_dim=-2, end_dim=-1)\n",
    "        K_r = torch.flatten(torch.stack((K_01, K_11), dim=-1), start_dim=-2, end_dim=-1)\n",
    "\n",
    "\n",
    "        # pattern : (B x H x T x T)\n",
    "        # heads : (B x T x H x d_h)\n",
    "        # out : (B x T x d)\n",
    "        mask = self.mask[:,:,:T,:T]\n",
    "        scores = Q_r @ K_r.transpose(-2, -1) * self.d_h ** -0.5     # ** -0.5 vs sqrt\n",
    "        scores = scores.masked_fill(mask, torch.finfo(scores.dtype).min)\n",
    "        pattern = torch.softmax(scores, -1)\n",
    "        heads = (pattern @ V_h).transpose(1, 2).contiguous()    # contiguous mem\n",
    "        out = torch.reshape(heads, (B, T, d))\n",
    "        return self.W_o(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "32252200",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.MHA = MultiHeadedAttention()\n",
    "        self.FFN = FeedForwardNetwork()\n",
    "        self.RMS1 = nn.RMSNorm(d)\n",
    "        self.RMS2 = nn.RMSNorm(d)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # X, X_n, X_out, Y, Y_n, Y_out, out : (B x T x d)\n",
    "        X_n = self.RMS1(X)\n",
    "        X_out = self.MHA(X_n)\n",
    "        Y = X + X_out\n",
    "\n",
    "        Y_n = self.RMS2(Y)\n",
    "        Y_out = self.FFN(Y_n)\n",
    "        out = Y + Y_out\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "542e2635",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    # TODO:\n",
    "    #   dropout\n",
    "    #   input params\n",
    "    #   LM head? (bias optional) X\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(N, d)\n",
    "        self.block = AttentionBlock()\n",
    "        self.RMS = nn.RMSNorm(d)\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.embedding(X)\n",
    "        out = self.block(out)\n",
    "        out = self.RMS(out)\n",
    "        out = out @ self.embedding.weight.T\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fdf2efdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200, 4098])\n",
      "tensor([[[  8.0043, -23.9523,   4.7797,  ...,  16.7017, -12.4647, -17.4723],\n",
      "         [ -4.6224,  17.3808,   7.3352,  ...,  25.2730,   0.0876,  16.3749],\n",
      "         [-20.0768, -27.5841,  18.2552,  ...,  43.3483,   9.3240,   8.4941],\n",
      "         ...,\n",
      "         [ 71.5850,  -8.3694, -26.9546,  ...,  30.9275,   6.8790,  60.5894],\n",
      "         [ -2.5121,  -4.8777, -62.1989,  ...,  16.8167,  74.4680, -32.2724],\n",
      "         [-37.1538,   5.4505, -22.2627,  ..., -15.7488,  20.9891,  33.6959]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer()\n",
    "X = torch.randint(N, (B, T))\n",
    "out = transformer(X)\n",
    "print(out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e67ae32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 8.4147e-01,  9.9998e-03,  1.0000e-04,  1.0000e-06],\n",
      "        [ 9.0930e-01,  1.9999e-02,  2.0000e-04,  2.0000e-06],\n",
      "        [ 1.4112e-01,  2.9995e-02,  3.0000e-04,  3.0000e-06],\n",
      "        [-7.5680e-01,  3.9989e-02,  4.0000e-04,  4.0000e-06],\n",
      "        [-9.5892e-01,  4.9979e-02,  5.0000e-04,  5.0000e-06]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.sin(torch.outer(torch.arange(6), RoPE_base ** (-2 * torch.arange(4) / 4))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e83996e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 5.],\n",
      "         [2., 6.],\n",
      "         [3., 7.],\n",
      "         [4., 8.]],\n",
      "\n",
      "        [[5., 1.],\n",
      "         [6., 2.],\n",
      "         [7., 3.],\n",
      "         [8., 4.]]])\n",
      "tensor([[1., 5., 2., 6., 3., 7., 4., 8.],\n",
      "        [5., 1., 6., 2., 7., 3., 8., 4.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "y = torch.Tensor([[5, 6, 7, 8], [1, 2, 3, 4]])\n",
    "print(torch.stack((x,y), dim=-1))\n",
    "print(torch.flatten(torch.stack((x,y), dim=-1), start_dim=-2, end_dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fcbc94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ashton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
